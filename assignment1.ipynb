{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, num_inputs, num_neurons, activation, weight_init):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_neurons = num_neurons\n",
    "        self.activation_fn = activation\n",
    "        self.weight_init = weight_init #what is this???\n",
    "\n",
    "        self.w = np.random.randn(self.num_neurons, self.num_inputs)\n",
    "        self.b = np.random.randn(self.num_neurons)\n",
    "\n",
    "    def activation(self,x):\n",
    "        if self.activation_fn == 'ReLU':\n",
    "            return np.maximum(0,x)\n",
    "        if self.activation_fn == 'softmax':\n",
    "            mx = np.max(x, axis = 1, keepdims=True)\n",
    "            x -= mx\n",
    "            # tp = np.sum(np.exp(x), axis=0, keepdims=True)\n",
    "            # print(tp)\n",
    "            return(np.exp(x)/np.sum(np.exp(x), axis=1, keepdims=True))\n",
    "        if self.activation_fn == 'sigmoid':\n",
    "            x = np.clip(x, -500, 500)\n",
    "            return(1/(1+np.exp(-x)))\n",
    "\n",
    "    def grad_activation(self, x):\n",
    "        if self.activation_fn == 'ReLU':\n",
    "            return 1*(x>0)\n",
    "        if self.activation_fn == 'sigmoid':\n",
    "            return (self.activation(x)*(1 - self.activation(x)))\n",
    "\n",
    "    def forward(self, cur_input):\n",
    "        re_bias = self.b.reshape(-1,1)\n",
    "        self.a = np.dot(self.w,cur_input.T) + re_bias\n",
    "        self.a = self.a.T\n",
    "        self.h = self.activation(self.a)\n",
    "        return self.h\n",
    "\n",
    "    def backward(self, grad_a, prev_a, prev_h, grad_activation):\n",
    "        self.dw = np.dot(grad_a.T, prev_h)\n",
    "        self.db = np.sum(grad_a, axis=0)\n",
    "        prev_h_grad = np.dot(grad_a, self.w)\n",
    "        der = grad_activation(prev_a)\n",
    "        grad_prev_a = prev_h_grad*der\n",
    "        return grad_prev_a\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, num_inputs, num_classes, num_hidden_layer, num_neurons, activation, weight_init):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_classes = num_classes\n",
    "        self.num_hidden_layer = num_hidden_layer\n",
    "        self.num_neurons = num_neurons\n",
    "        self.activation = activation\n",
    "        self.weight_init = weight_init\n",
    "        self.layers = []\n",
    "        self.layers.append(Layer(num_inputs, num_neurons, activation, weight_init))\n",
    "        for i in range(num_hidden_layer - 1):\n",
    "            self.layers.append(Layer(num_neurons, num_neurons, 'ReLU', weight_init))\n",
    "        self.layers.append(Layer(num_neurons, num_classes, 'softmax', weight_init))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        cur_in = inputs\n",
    "        for i in range(self.num_hidden_layer+1):\n",
    "            cur_out = self.layers[i].forward(cur_in)\n",
    "            cur_in = cur_out\n",
    "        self.y_pred = cur_out\n",
    "        return cur_out\n",
    "\n",
    "    def backward(self, outputs):\n",
    "        grad_a_L = -(outputs - self.y_pred)\n",
    "        for i in range(self.num_hidden_layer, 0, -1):\n",
    "            grad_a_L = self.layers[i].backward(grad_a_L,self.layers[i-1].a,self.layers[i-1].h, self.layers[i-1].grad_activation)\n",
    "\n",
    "        self.layers[0].dw = np.dot(grad_a_L.T, self.inputs)\n",
    "        self.layers[0].db = np.sum(grad_a_L, axis=0)\n",
    "        \n",
    "    def minibatch_sgd(self, dw, db, eta : float = 0.01):\n",
    "            for j in range(self.num_hidden_layer+1):\n",
    "                self.layers[j].w -= eta*dw[j]\n",
    "                self.layers[j].b -= eta*db[j]\n",
    "\n",
    "    def momentum_gd(self, uw, ub, dw, db, eta : float = 0.01, beta : float = 0.9):\n",
    "        for j in range(self.num_hidden_layer+1):\n",
    "            uw[j] = beta*uw[j] + dw[j]\n",
    "            ub[j] = beta*ub[j] + db[j]\n",
    "            self.layers[j].w -= eta*uw[j]\n",
    "            self.layers[j].b -= eta*ub[j]\n",
    "        return uw, ub\n",
    "\n",
    "    def NAG_gd(self, mw, mb, dw, db, eta : float = 0.01, beta : float = 0.9):\n",
    "        for j in range(self.num_hidden_layer+1):\n",
    "            mw[j] = beta*mw[j] + dw[j]\n",
    "            mb[j] = beta*mb[j] + db[j]\n",
    "            self.layers[j].w -= eta*(beta*mw[j] + dw[j])\n",
    "            self.layers[j].b -= eta*(beta*mb[j] + db[j])\n",
    "        return mw, mb\n",
    "\n",
    "    def RMSProp_gd(self, uw, ub, dw, db, eta : float = 0.01, beta : float = 0.9, epsilon : float = 1e-8):\n",
    "        for j in range(self.num_hidden_layer+1):\n",
    "            uw[j] = beta*uw[j] + (1-beta)*dw[j]**2\n",
    "            ub[j] = beta*ub[j] + (1-beta)*db[j]**2\n",
    "            self.layers[j].w -= eta*dw[j]/(np.sqrt(uw[j])+epsilon)\n",
    "            self.layers[j].b -= eta*db[j]/(np.sqrt(ub[j])+epsilon)\n",
    "        return uw, ub\n",
    "    \n",
    "    def Adam_gd(self, mw, mb, uw, ub, dw, db, t, eta : float = 0.01, beta1 : float = 0.9, beta2 : float = 0.999, epsilon : float = 1e-8):\n",
    "        for j in range(self.num_hidden_layer+1):\n",
    "            mw[j] = beta1*mw[j] + (1-beta1)*dw[j]\n",
    "            mb[j] = beta1*mb[j] + (1-beta1)*db[j]\n",
    "            uw[j] = beta2*uw[j] + (1-beta2)*(dw[j]**2)\n",
    "            ub[j] = beta2*ub[j] + (1-beta2)*(db[j]**2)\n",
    "            mw_hat = mw[j]/(1-beta1**t)\n",
    "            mb_hat = mb[j]/(1-beta1**t)\n",
    "            uw_hat = uw[j]/(1-beta2**t)\n",
    "            ub_hat = ub[j]/(1-beta2**t)\n",
    "            self.layers[j].w -= eta*mw_hat/(np.sqrt(uw_hat)+epsilon)\n",
    "            self.layers[j].b -= eta*mb_hat/(np.sqrt(ub_hat)+epsilon)\n",
    "        return mw, mb, uw, ub\n",
    "\n",
    "\n",
    "    def NAdam_gd(self, mw, mb, uw, ub, dw, db, t, eta : float = 0.01, beta1 : float = 0.9, beta2 : float = 0.999, epsilon : float = 1e-8):\n",
    "        for j in range(self.num_hidden_layer+1):\n",
    "            mw[j] = beta1*mw[j] + (1-beta1)*dw[j]\n",
    "            mb[j] = beta1*mb[j] + (1-beta1)*db[j]\n",
    "            uw[j] = beta2*uw[j] + (1-beta2)*dw[j]**2\n",
    "            ub[j] = beta2*ub[j] + (1-beta2)*db[j]**2\n",
    "            m_w_hat = mw[j]/(1-np.power(beta1, t+1))\n",
    "            m_b_hat = mb[j]/(1-np.power(beta1, t+1))\n",
    "            uw_hat = uw[j]/(1-np.power(beta2, t+1))\n",
    "            ub_hat = ub[j]/(1-np.power(beta2, t+1))\n",
    "            self.layers[j].w -= (eta/(np.sqrt(uw_hat) + epsilon))*(beta1*m_w_hat+ (1-beta1)*dw[j]/(1-np.power(beta1, t+1)))\n",
    "            self.layers[j].b -= (eta/(np.sqrt(ub_hat) + epsilon))*(beta1*m_b_hat + (1-beta1)*db[j]/(1-np.power(beta1, t+1)))\n",
    "        return mw, mb, uw, ub\n",
    "\n",
    "\n",
    "    def train(self, X_train, y_train, batch_size, epochs, optimizer, eta : float = 0.001, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8):\n",
    "        for i in range(epochs):\n",
    "            uw = [np.zeros_like(self.layers[j].w) for j in range(self.num_hidden_layer+1)]\n",
    "            ub = [np.zeros_like(self.layers[j].b) for j in range(self.num_hidden_layer+1)]\n",
    "            mw = [np.zeros_like(self.layers[j].w) for j in range(self.num_hidden_layer+1)]\n",
    "            mb = [np.zeros_like(self.layers[j].b) for j in range(self.num_hidden_layer+1)]\n",
    "            t = 1\n",
    "            for i in tqdm(range(0, X_train.shape[0], batch_size)):\n",
    "                x = X_train[i:i+batch_size]\n",
    "                y = y_train[i:i+batch_size]\n",
    "                self.forward(x)\n",
    "                self.backward(y)\n",
    "                dw = [self.layers[j].dw / X_train.shape[0] for j in range(self.num_hidden_layer+1)]\n",
    "                db = [self.layers[j].db / X_train.shape[0] for j in range(self.num_hidden_layer+1)]\n",
    "                if optimizer == \"minibatch_sgd\":\n",
    "                    self.minibatch_sgd(dw, db, eta, batch_size)\n",
    "                elif optimizer == \"momentum_gd\":\n",
    "                    uw, ub = self.momentum_gd(uw, ub,dw, db, eta, beta1)\n",
    "                elif optimizer == \"NAG_gd\":\n",
    "                    mw, mb = self.NAG_gd(mw, mb, dw, db, eta, beta1)\n",
    "                elif optimizer == \"RMSProp_gd\":\n",
    "                    uw, ub = self.RMSProp_gd(uw, ub, dw, db, eta, beta1, epsilon)\n",
    "                elif optimizer == \"Adam_gd\":\n",
    "                    mw, mb, uw, ub = self.Adam_gd(mw, mb, uw, ub, dw, db, t, eta, beta1, beta2, epsilon)\n",
    "                elif optimizer == \"NAdam_gd\":\n",
    "                    mw, mb, uw, ub = self.NAdam_gd(mw, mb, uw, ub, dw, db, t, eta, beta1, beta2, epsilon)   \n",
    "                t += 1\n",
    "            self.test(X_train, y_train) \n",
    "                    \n",
    "\n",
    "    def test(self, X_test, y_test):\n",
    "        self.forward(X_test)\n",
    "        y_pred = self.layers[-1].a\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "        print(np.sum(y_pred == y_test)/y_test.shape[0])\n",
    "\n",
    "    def cross_entropy(self, y_pred, y):\n",
    "        return -np.sum(y*np.log(y_pred))/ y_pred.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(784, 10, 1, 64, 'ReLU', 'random')\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "y_train = np.eye(10)[y_train]\n",
    "y_test = np.eye(10)[y_test]\n",
    "\n",
    "nn.minibatch_sgd(X_train, y_train, eta=0.01, batch_size=25)\n",
    "nn.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # momentum based gradient descent\n",
    "# def train_momentum(self, X_train, y_train, epochs, learning_rate, gamma):\n",
    "#     for i in range(epochs):\n",
    "#         dw, db = 0, 0\n",
    "#         for x,y in zip(X_train,y_train):\n",
    "#             self.forward(x)\n",
    "#             self.backward(y)\n",
    "#             dw += self.layers[0].grad_w\n",
    "#             db += self.layers[0].grad_b\n",
    "#         for k in range(self.num_hidden_layer+1):\n",
    "#             uw[k] = gamma*prev_uw + learning_rate*dw\n",
    "#             ub = gamma*prev_ub + learning_rate*db\n",
    "#             self.layers[k].weights -= uw\n",
    "#             self.layers[k].biases -= ub\n",
    "#         prev_uw = uw\n",
    "#         prev_ub = ub\n",
    "\n",
    "# class MGD(Optimiser):\n",
    "# \tdef __init__(self, model : Model = None, learning_rate : float = 0.01, weight_decay : float = 0.0, momentum : float = 0.9):\n",
    "# \t\tsuper().__init__(model, learning_rate, weight_decay)\n",
    "# \t\tself.momentum = momentum\n",
    "# \t\tself.u_w = [np.zeros_like(self.model.layers[i].weights) for i in range(len(self.model.layers))]\n",
    "# \t\tself.u_b = [np.zeros_like(self.model.layers[i].bias) for i in range(len(self.model.layers))]\n",
    "\n",
    "# \tdef step(self) :\n",
    "# \t\ti = 0\n",
    "# \t\tfor layer in self.model.layers:\n",
    "# \t\t\tself.u_w[i] = self.momentum * self.u_w[i] + layer.grad_w\n",
    "# \t\t\tself.u_b[i] = self.momentum * self.u_b[i] + layer.grad_b\n",
    "# \t\t\tlayer.weights -= self.learning_rate * (self.u_w[i] + self.weight_decay * layer.weights)\n",
    "# \t\t\tlayer.bias -= self.learning_rate * (self.u_b[i] + self.weight_decay * layer.bias)\n",
    "# \t\t\ti += 1\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "# def do_stochastic_gradient_descent():\n",
    "  \n",
    "#   w,b,eta,max_epochs = -2,-2,1.0,1000\n",
    "  \n",
    "#   for i in range(max_epochs):\n",
    "#     dw,db = 0,0\n",
    "#     for x,y in zip(X,Y):\n",
    "#       dw += grad_w(x,w,b,y)\n",
    "#       db += grad_b(x,w,b,y)    \n",
    "#       w = w - eta*dw\n",
    "#       b = b - eta*db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5efca79e2833d4ffb0354f946a6976ef19c76d0dd9e1a870ddb2585fd6af110f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
