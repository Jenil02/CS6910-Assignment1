{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from keras.datasets import fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = fashion_mnist.load_data()[1][1]\n",
    "X = fashion_mnist.load_data()[1][0]\n",
    "fig, ax = plt.subplots(2,5)\n",
    "for i, ax in enumerate(ax.flatten()):\n",
    "    im_idx = np.argwhere(y == i)[0]\n",
    "    plottable_image = np.reshape(X[im_idx], (28, 28))\n",
    "    ax.imshow(plottable_image, cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, num_inputs, num_neurons, activation, weight_init):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_neurons = num_neurons\n",
    "        self.activation_fn = activation\n",
    "        self.weight_init = weight_init\n",
    "        self.w = np.random.randn(self.num_neurons, self.num_inputs)\n",
    "        self.b = np.random.randn(self.num_neurons)\n",
    "        if weight_init == 'Xavier':\n",
    "            self.w = self.w/np.sqrt(self.num_inputs)\n",
    "            self.b = self.b/np.sqrt(self.num_inputs)\n",
    "\n",
    "    def activation(self,x):\n",
    "        if self.activation_fn == 'ReLU':\n",
    "            return np.maximum(0,x)\n",
    "        if self.activation_fn == 'softmax':\n",
    "            mx = np.max(x, axis = 1, keepdims=True)\n",
    "            x -= mx\n",
    "            return(np.exp(x)/np.sum(np.exp(x), axis=1, keepdims=True))\n",
    "        if self.activation_fn == 'sigmoid':\n",
    "            x = np.clip(x, -500, 500)\n",
    "            return(1/(1+np.exp(-x)))\n",
    "        if self.activation_fn == 'tanh':\n",
    "            return np.tanh(x)\n",
    "\n",
    "    def grad_activation(self, x):\n",
    "        if self.activation_fn == 'ReLU':\n",
    "            return 1*(x>0)\n",
    "        if self.activation_fn == 'sigmoid':\n",
    "            return (self.activation(x)*(1 - self.activation(x)))\n",
    "        if self.activation_fn == 'tanh':\n",
    "            return (1 - np.square(self.activation(x)))\n",
    "\n",
    "    def forward(self, cur_input):\n",
    "        re_bias = self.b.reshape(-1,1)\n",
    "        self.a = np.dot(self.w,cur_input.T) + re_bias\n",
    "        self.a = self.a.T\n",
    "        self.h = self.activation(self.a)\n",
    "        return self.h\n",
    "\n",
    "    def backward(self, grad_a, prev_a, prev_h, grad_activation):\n",
    "        self.dw = np.dot(grad_a.T, prev_h)\n",
    "        self.db = np.sum(grad_a, axis=0)\n",
    "        prev_h_grad = np.dot(grad_a, self.w)\n",
    "        der = grad_activation(prev_a)\n",
    "        grad_prev_a = prev_h_grad*der\n",
    "        return grad_prev_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, num_inputs, num_classes, num_hidden_layer, num_neurons, activation, weight_init):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_classes = num_classes\n",
    "        self.num_hidden_layer = num_hidden_layer\n",
    "        self.num_neurons = num_neurons\n",
    "        self.activation = activation\n",
    "        self.weight_init = weight_init\n",
    "        self.layers = []\n",
    "        self.layers.append(Layer(num_inputs, num_neurons, activation, weight_init))\n",
    "        for i in range(num_hidden_layer - 1):\n",
    "            self.layers.append(Layer(num_neurons, num_neurons, 'ReLU', weight_init))\n",
    "        self.layers.append(Layer(num_neurons, num_classes, 'softmax', weight_init))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        cur_in = inputs\n",
    "        for i in range(self.num_hidden_layer+1):\n",
    "            cur_out = self.layers[i].forward(cur_in)\n",
    "            cur_in = cur_out\n",
    "        self.y_pred = cur_out\n",
    "        return cur_out\n",
    "\n",
    "    def backward(self, outputs):\n",
    "        grad_a_L = -(outputs - self.y_pred)\n",
    "        for i in range(self.num_hidden_layer, 0, -1):\n",
    "            grad_a_L = self.layers[i].backward(grad_a_L,self.layers[i-1].a,self.layers[i-1].h, self.layers[i-1].grad_activation)\n",
    "\n",
    "        self.layers[0].dw = np.dot(grad_a_L.T, self.inputs)\n",
    "        self.layers[0].db = np.sum(grad_a_L, axis=0)\n",
    "        \n",
    "    def minibatch_sgd(self, dw, db, eta : float = 0.01, weight_decay : float = 0.0):\n",
    "            for j in range(self.num_hidden_layer+1):\n",
    "                self.layers[j].w -= eta*dw[j] + eta*weight_decay*self.layers[j].w\n",
    "                self.layers[j].b -= eta*db[j] + eta*weight_decay*self.layers[j].b\n",
    "\n",
    "    def momentum_gd(self, uw, ub, dw, db, eta : float = 0.01, weight_decay : float = 0.0, beta : float = 0.9):\n",
    "        for j in range(self.num_hidden_layer+1):\n",
    "            uw[j] = beta*uw[j] + dw[j]\n",
    "            ub[j] = beta*ub[j] + db[j] \n",
    "            self.layers[j].w -= eta*uw[j] + eta*weight_decay*self.layers[j].w\n",
    "            self.layers[j].b -= eta*ub[j] + eta*weight_decay*self.layers[j].b\n",
    "        return uw, ub\n",
    "\n",
    "    def NAG_gd(self, mw, mb, dw, db, eta : float = 0.01, weight_decay : float = 0.0, beta : float = 0.9):\n",
    "        for j in range(self.num_hidden_layer+1):\n",
    "            mw[j] = beta*mw[j] + dw[j]\n",
    "            mb[j] = beta*mb[j] + db[j]\n",
    "            self.layers[j].w -= eta*(beta*mw[j] + dw[j]) + eta*weight_decay*self.layers[j].w\n",
    "            self.layers[j].b -= eta*(beta*mb[j] + db[j]) + eta*weight_decay*self.layers[j].b\n",
    "        return mw, mb\n",
    "\n",
    "    def RMSProp_gd(self, uw, ub, dw, db, eta : float = 0.01, weight_decay : float = 0.0, beta : float = 0.9, epsilon : float = 1e-8):\n",
    "        for j in range(self.num_hidden_layer+1):\n",
    "            uw[j] = beta*uw[j] + (1-beta)*dw[j]**2\n",
    "            ub[j] = beta*ub[j] + (1-beta)*db[j]**2\n",
    "            self.layers[j].w -= eta*dw[j]/(np.sqrt(uw[j])+epsilon) + eta*weight_decay*self.layers[j].w\n",
    "            self.layers[j].b -= eta*db[j]/(np.sqrt(ub[j])+epsilon) + eta*weight_decay*self.layers[j].b\n",
    "        return uw, ub\n",
    "    \n",
    "    def Adam_gd(self, mw, mb, uw, ub, dw, db, t, eta : float = 0.01, weight_decay : float = 0.0, beta1 : float = 0.9, beta2 : float = 0.999, epsilon : float = 1e-8):\n",
    "        for j in range(self.num_hidden_layer+1):\n",
    "            mw[j] = beta1*mw[j] + (1-beta1)*dw[j]\n",
    "            mb[j] = beta1*mb[j] + (1-beta1)*db[j]\n",
    "            uw[j] = beta2*uw[j] + (1-beta2)*(dw[j]**2)\n",
    "            ub[j] = beta2*ub[j] + (1-beta2)*(db[j]**2)\n",
    "            mw_hat = mw[j]/(1-beta1**t)\n",
    "            mb_hat = mb[j]/(1-beta1**t)\n",
    "            uw_hat = uw[j]/(1-beta2**t)\n",
    "            ub_hat = ub[j]/(1-beta2**t)\n",
    "            self.layers[j].w -= eta*mw_hat/(np.sqrt(uw_hat)+epsilon) + eta*weight_decay*self.layers[j].w\n",
    "            self.layers[j].b -= eta*mb_hat/(np.sqrt(ub_hat)+epsilon) + eta*weight_decay*self.layers[j].b\n",
    "        return mw, mb, uw, ub\n",
    "\n",
    "\n",
    "    def NAdam_gd(self, mw, mb, uw, ub, dw, db, t, eta : float = 0.01, weight_decay : float = 0.0, beta1 : float = 0.9, beta2 : float = 0.999, epsilon : float = 1e-8):\n",
    "        for j in range(self.num_hidden_layer+1):\n",
    "            mw[j] = beta1*mw[j] + (1-beta1)*dw[j]\n",
    "            mb[j] = beta1*mb[j] + (1-beta1)*db[j]\n",
    "            uw[j] = beta2*uw[j] + (1-beta2)*dw[j]**2\n",
    "            ub[j] = beta2*ub[j] + (1-beta2)*db[j]**2\n",
    "            m_w_hat = mw[j]/(1-np.power(beta1, t+1))\n",
    "            m_b_hat = mb[j]/(1-np.power(beta1, t+1))\n",
    "            uw_hat = uw[j]/(1-np.power(beta2, t+1))\n",
    "            ub_hat = ub[j]/(1-np.power(beta2, t+1))\n",
    "            self.layers[j].w -= (eta/(np.sqrt(uw_hat) + epsilon))*(beta1*m_w_hat+ (1-beta1)*dw[j]/(1-np.power(beta1, t+1))) + eta*weight_decay*self.layers[j].w\n",
    "            self.layers[j].b -= (eta/(np.sqrt(ub_hat) + epsilon))*(beta1*m_b_hat + (1-beta1)*db[j]/(1-np.power(beta1, t+1))) + eta*weight_decay*self.layers[j].b\n",
    "        return mw, mb, uw, ub\n",
    "\n",
    "\n",
    "    def train(self, X_train, y_train, X_test, y_test, batch_size, epochs, optimizer, eta : float = 0.001, weight_decay : float = 0.0, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8):\n",
    "        for i in range(epochs):\n",
    "            uw = [np.zeros_like(self.layers[j].w) for j in range(self.num_hidden_layer+1)]\n",
    "            ub = [np.zeros_like(self.layers[j].b) for j in range(self.num_hidden_layer+1)]\n",
    "            mw = [np.zeros_like(self.layers[j].w) for j in range(self.num_hidden_layer+1)]\n",
    "            mb = [np.zeros_like(self.layers[j].b) for j in range(self.num_hidden_layer+1)]\n",
    "            t = 1\n",
    "            for i in tqdm(range(0, X_train.shape[0], batch_size)):\n",
    "                x = X_train[i:i+batch_size]\n",
    "                y = y_train[i:i+batch_size]\n",
    "                self.forward(x)\n",
    "                self.backward(y)\n",
    "                dw = [self.layers[j].dw / X_train.shape[0] for j in range(self.num_hidden_layer+1)]\n",
    "                db = [self.layers[j].db / X_train.shape[0] for j in range(self.num_hidden_layer+1)]\n",
    "                if optimizer == \"minibatch_sgd\":\n",
    "                    self.minibatch_sgd(dw, db, eta)\n",
    "                elif optimizer == \"momentum_gd\":\n",
    "                    uw, ub = self.momentum_gd(uw, ub,dw, db, eta, weight_decay, beta1)\n",
    "                elif optimizer == \"NAG_gd\":\n",
    "                    mw, mb = self.NAG_gd(mw, mb, dw, db, eta, weight_decay, beta1)\n",
    "                elif optimizer == \"RMSProp_gd\":\n",
    "                    uw, ub = self.RMSProp_gd(uw, ub, dw, db, eta, weight_decay, beta1, epsilon)\n",
    "                elif optimizer == \"Adam_gd\":\n",
    "                    mw, mb, uw, ub = self.Adam_gd(mw, mb, uw, ub, dw, db, t, eta, weight_decay, beta1, beta2, epsilon)\n",
    "                elif optimizer == \"NAdam_gd\":\n",
    "                    mw, mb, uw, ub = self.NAdam_gd(mw, mb, uw, ub, dw, db, t, eta, weight_decay, beta1, beta2, epsilon)   \n",
    "                t += 1\n",
    "            train_acc, train_loss = self.test(X_train, y_train)\n",
    "            test_acc, test_loss = self.test(X_test, y_test)\n",
    "            wandb.log({\"train_acc\": train_acc, \"train_loss\": train_loss, \"val_acc\": test_acc, \"val_loss\": test_loss})\n",
    "                    \n",
    "\n",
    "    def test(self, X_test, y_test):\n",
    "        self.forward(X_test)\n",
    "        y_pred = self.layers[-1].h\n",
    "        loss = self.cross_entropy(y_pred, y_test)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "        return np.sum(y_pred == y_test)/y_test.shape[0], loss\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_true):\n",
    "        return -np.sum(y_true*np.log(y_pred + 1e-9))/y_pred.shape[0]\n",
    "    \n",
    "    def square_loss(self, y_pred, y_true):\n",
    "        return np.sum((y_pred - y_true)**2)/y_pred.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = NeuralNetwork(784, 10, 1, 64, 'ReLU', 'random')\n",
    "# (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# X_train = X_train.reshape(X_train.shape[0], 784)\n",
    "# X_test = X_test.reshape(X_test.shape[0], 784)\n",
    "\n",
    "# X_train /= 255\n",
    "# X_test /= 255\n",
    "\n",
    "# y_train = np.eye(10)[y_train]\n",
    "# y_test = np.eye(10)[y_test]\n",
    "\n",
    "# nn.minibatch_sgd(X_train, y_train, eta=0.01, batch_size=25)\n",
    "# nn.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random', #grid, random\n",
    "    'metric': {\n",
    "        'name': 'val_acc',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'optimizer': {\n",
    "            'values': ['minibatch_sgd', 'momentum_gd', 'NAG_gd', 'RMSProp_gd', 'Adam_gd', 'NAdam_gd']\n",
    "        },\n",
    "        'eta': {\n",
    "            'values': [0.01, 0.001, 0.0001]\n",
    "        },\n",
    "        'beta1': {\n",
    "            'values': [0.9]\n",
    "        },\n",
    "        'beta2': {\n",
    "            'values': [0.999]\n",
    "        },\n",
    "        'epsilon': {\n",
    "            'values': [1e-8, 1e-7, 1e-6]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [32, 64, 128]\n",
    "        },\n",
    "        'epochs': {\n",
    "            'values': [5, 10, 15]\n",
    "        },\n",
    "        'num_hidden_layer': {\n",
    "            'values': [1, 2, 3]\n",
    "        },\n",
    "        'num_hidden_unit': {\n",
    "            'values': [32, 64, 128]\n",
    "        },\n",
    "        'activation': {\n",
    "            'values': ['ReLU', 'sigmoid', 'tanh']\n",
    "        },\n",
    "        'initializer': {\n",
    "            'values': ['Xavier', 'random']\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0.0, 0.0001, 0.5]\n",
    "        },\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.1, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    idx = np.arange(X.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "    split = int(X.shape[0]*test_size)\n",
    "    X_train, X_test = X[split:], X[:split]\n",
    "    y_train, y_test = y[split:], y[:split]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the W&B Python Library and log into W&B\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "def get_name(config):\n",
    "    name = \"\"\n",
    "    for key in config.keys():\n",
    "        name += key + \"_\" + str(config[key]) + \"-\"\n",
    "    return name[:-1]\n",
    "\n",
    "# 1: Define objective/training function\n",
    "def objective(config):\n",
    "    # Load data\n",
    "    wandb.run.name = get_name(config)\n",
    "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1) / 255\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) / 255\n",
    "    y_train = np.eye(10)[y_train]\n",
    "    y_test = np.eye(10)[y_test]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Train model\n",
    "    nn = NeuralNetwork(784, 10, config.num_hidden_layer, config.num_hidden_unit, config.activation, config.initializer)\n",
    "    nn.train(X_train, y_train, X_test, y_test, config.batch_size, config.epochs, config.optimizer, config.eta, config.beta1, config.beta2, config.epsilon)\n",
    "\n",
    "def main():\n",
    "    wandb.init(project='new-proj')\n",
    "    score = objective(wandb.config)\n",
    "\n",
    "sweep_configuration = sweep_config\n",
    "\n",
    "# 3: Start the sweep\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project='new-proj')\n",
    "wandb.agent(sweep_id, function=main, count=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5efca79e2833d4ffb0354f946a6976ef19c76d0dd9e1a870ddb2585fd6af110f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
